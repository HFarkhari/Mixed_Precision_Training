{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e99683",
   "metadata": {},
   "source": [
    "# Mixed Precision Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce6218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e62e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64_000\n",
    "D_in   = 10240\n",
    "D_out  = 5120\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31561751",
   "metadata": {},
   "source": [
    "# Apex.amp, deprecated, should use PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6c230",
   "metadata": {},
   "source": [
    "DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5cfd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbf3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(N, D_in, device=\"cuda\")\n",
    "y = torch.randn(N, D_out, device=\"cuda\")\n",
    "model = torch.nn.Linear(D_in, D_out).cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7992c867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.38 sec\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss: # changes\n",
    "        scaled_loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    \n",
    "ed = time.time()\n",
    "print(f'time: {np.round((ed-st),2)} sec')\n",
    "\n",
    "# 13G GPU, 22 sec\n",
    "# 01/02(mixed) --> 22 sec\n",
    "# 03 (fp16) --> 9sec\n",
    "# 00 (fp32) --> error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4ace0",
   "metadata": {},
   "source": [
    "# Automatic Mixed Precision package - torch.amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e53686",
   "metadata": {},
   "source": [
    "Refrences:\n",
    "\n",
    "https://pytorch.org/docs/stable/amp.html\n",
    "\n",
    "https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c1881",
   "metadata": {},
   "source": [
    "torch.autocast(\"cuda\", args...)   ==   torch.cuda.amp.autocast(args...).\n",
    "\n",
    "torch.autocast(\"cpu\", args...) == torch.cpu.amp.autocast(args...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c46e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2665504",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64_000\n",
    "D_in   = 10240\n",
    "D_out  = 5120\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34f2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler #, autocast\n",
    "from torch import autocast\n",
    "\n",
    "max_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a88ae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = torch.device(\"cuda\") if \\\n",
    "#     torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05fa62-15d1-40c0-91a7-10941b84f5c5",
   "metadata": {},
   "source": [
    "refrence: https://pytorch.org/docs/stable/amp.html\n",
    "\n",
    "Ordinarily, “automatic mixed precision training” with datatype of torch.float16 uses torch.autocast and torch.cuda.amp.GradScaler together, as shown in the CUDA Automatic Mixed Precision examples and CUDA Automatic Mixed Precision recipe. However, torch.autocast and torch.cuda.amp.GradScaler are modular, and may be used separately if desired. As shown in the CPU example section of torch.autocast, “automatic mixed precision training/inference” on CPU with datatype of torch.bfloat16 only uses torch.autocast.\n",
    "\n",
    "torch.float16  -->      uses torch.autocast and torch.cuda.amp.GradScaler together,\n",
    "\n",
    "torch.bfloat16 --> only uses torch.autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63e7f9f-6871-4737-b1c9-1b82f6db0b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e88fd9-387c-49f1-b53a-3501096aa376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hint\n",
    "dtype == 'bfloat16',  dtype == torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2340178d-a9ba-4508-8da7-2f9c4fd3d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bfloat16, scaler will be disabled\n",
    "scaler = GradScaler(enabled=(dtype == torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0cc447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# @autocast()\n",
    "# def func():\n",
    "\n",
    "class AutocastModel(torch.nn.Module):\n",
    "    ...\n",
    "    @autocast(device)\n",
    "    def forward(self, input):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7004b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(N, D_in).to(device)\n",
    "y = torch.rand(N, D_out).to(device)\n",
    "\n",
    "# Creates model and optimizer in default precision\n",
    "model     = torch.nn.Linear(D_in, D_out).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "219dc95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.62 sec\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "for t in np.arange(epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Enables autocasting for the forward pass (model + loss)\n",
    "    with autocast(device_type=device, enabled=True, dtype=dtype): # torch.float16,  torch.bfloat16\n",
    "        y_pred = model(x)\n",
    "        loss = torch.nn.functional.mse_loss(y_pred.float(), y)\n",
    "\n",
    "##### Exits the context manager before backward()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "ed = time.time()\n",
    "print(f'time: {np.round((ed-st),2)} sec')\n",
    "\n",
    "# 12.8G GPU, 21.8 sec\n",
    "# disable fp16 --> 38.2 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b28b16",
   "metadata": {},
   "source": [
    "# Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb43e3e3",
   "metadata": {},
   "source": [
    "======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.\n",
    "\n",
    "Check your Nvidia GPUs Compute Capability:\n",
    "https://developer.nvidia.com/cuda-gpus#compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc67c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.\n",
      "                  Use NVIDIA Nsight Systems for GPU tracing and CPU sampling and NVIDIA Nsight Compute for GPU profiling.\n",
      "                  Refer https://developer.nvidia.com/tools-overview for more details.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvprof python pytorch_mixed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba91665b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: nvprof [options] [application] [application-arguments]\n",
      "Options:\n",
      "       --aggregate-mode <on|off>\n",
      "                        Turn on/off aggregate mode for events and metrics specified\n",
      "                        by subsequent \"--events\" and \"--metrics\" options. Those\n",
      "                        event/metric values will be collected for each domain instance,\n",
      "                        instead of the whole device. Allowed values:\n",
      "                        \ton - turn on aggregate mode (default)\n",
      "                        \toff - turn off aggregate mode\n",
      "\n",
      "       --analysis-metrics\n",
      "                        Collect profiling data that can be imported to Visual Profiler's\n",
      "                        \"analysis\" mode. Note: Use \"--export-profile\" to specify\n",
      "                        an export file.\n",
      "\n",
      "       --annotate-mpi <off|openmpi|mpich>\n",
      "                        Automatically annotate MPI calls with NVTX markers. Specify\n",
      "                        the MPI implementation installed on your machine. Currently,\n",
      "                        Open MPI and MPICH implementations are supported. By default,\n",
      "                        this option is off.\n",
      "\n",
      "       --concurrent-kernels <on|off>\n",
      "                        Turn on/off concurrent kernel execution. If concurrent kernel\n",
      "                        execution is off, all kernels running on one device will\n",
      "                        be serialized. Allowed values:\n",
      "                        \ton - turn on concurrent kernel execution (default)\n",
      "                        \toff - turn off concurrent kernel execution\n",
      "\n",
      "       --continuous-sampling-interval <interval>\n",
      "                        Set the continuous mode sampling interval in milliseconds.\n",
      "                        Minimum is 1 ms. Default is 2 ms.\n",
      "\n",
      "       --cpu-thread-tracing <on|off>\n",
      "                        Collect information about CPU thread API activity.\n",
      "                        Allowed values:\n",
      "                        \ton  - turn on CPU thread API tracing\n",
      "                        \toff - turn off CPU thread API tracing (default)\n",
      "\n",
      "       --dependency-analysis\n",
      "                        Generate event dependency graph for host and device activities\n",
      "                        and run dependency analysis.\n",
      "\n",
      "       --device-buffer-size <size in MBs>\n",
      "                        Set the device memory size (in MBs) reserved for storing\n",
      "                        profiling data for non-CDP operations, especially for concurrent\n",
      "                        kernel tracing, for each buffer on a context. The default\n",
      "                        value is 8MB. The size should be a positive integer.\n",
      "\n",
      "       --device-cdp-buffer-size <size in MBs>\n",
      "                        Set the device memory size (in MBs) reserved for storing\n",
      "                        profiling data for CDP operations for each buffer on a context.\n",
      "                        The default value is 8MB. The size should be a positive\n",
      "                        integer.\n",
      "\n",
      "       --devices <device ids>\n",
      "                        Change the scope of subsequent \"--events\", \"--metrics\", \"--query-events\"\n",
      "                        and \"--query-metrics\" options.\n",
      "                        Allowed values:\n",
      "                        \tall - change scope to all valid devices\n",
      "                        \tcomma-separated device IDs - change scope to specified\n",
      "                        devices\n",
      "\n",
      "       --event-collection-mode <mode>\n",
      "                        Choose event collection mode for all events/metrics Allowed\n",
      "                        values:\n",
      "                        \tkernel - events/metrics are collected only for durations\n",
      "                        of kernel executions (default)\n",
      "                        \tcontinuous - events/metrics are collected for duration\n",
      "                        of application. This is not applicable for non-tesla devices.\n",
      "                        This mode is compatible only with NVLink metrics. This mode\n",
      "                        is incompatible with \"--profile-all-processes\" or \"--profile-child-processes\"\n",
      "                        or \"--replay-mode kernel\" or \"--replay-mode application\".\n",
      "\n",
      "  -e,  --events <event names>\n",
      "                        Specify the events to be profiled on certain device(s). Multiple\n",
      "                        event names separated by comma can be specified. Which device(s)\n",
      "                        are profiled is controlled by the \"--devices\" option. Otherwise\n",
      "                        events will be collected on all devices.\n",
      "                        For a list of available events, use \"--query-events\".\n",
      "                        Use \"--events all\" to profile all events available for each\n",
      "                        device.\n",
      "                        Use \"--devices\" and \"--kernels\" to select a specific kernel\n",
      "                        invocation.\n",
      "\n",
      "       --kernel-latency-timestamps <on|off>\n",
      "                        Turn on/off collection of kernel latency timestamps, namely\n",
      "                        queued and submitted. The queued timestamp is captured when\n",
      "                        a kernel launch command was queued into the CPU command\n",
      "                        buffer. The submitted timestamp denotes when the CPU command\n",
      "                        buffer containing this kernel launch was submitted to the\n",
      "                        GPU. Turning this option on may incur an overhead during\n",
      "                        profiling. Allowed values:\n",
      "                        \ton - turn on collection of kernel latency timestamps\n",
      "                        \toff - turn off collection of kernel latency timestamps\n",
      "                        (default)\n",
      "\n",
      "       --kernels <kernel path syntax>\n",
      "                        Change the scope of subsequent \"--events\", \"--metrics\" options.\n",
      "                        The syntax is as follows:\n",
      "                        \t<kernel name>\n",
      "                        \tLimit scope to given kernel name.\n",
      "                        or\n",
      "                        \t<context id/name>:<stream id/name>:<kernel name>:<invocation>\n",
      "                        The context/stream IDs, names, kernel name and invocation\n",
      "                        can be regular expressions. Empty string matches any number\n",
      "                        or characters. If <context id/name> or <stream id/name>\n",
      "                        is a positive number, it's strictly matched against the\n",
      "                        CUDA context/stream ID. Otherwise it's treated as a regular\n",
      "                        expression and matched against the context/stream name specified\n",
      "                        by the NVTX library. If the invocation count is a positive\n",
      "                        number, it's strictly matched against the invocation of\n",
      "                        the kernel. Otherwise it's treated as a regular expression.\n",
      "                        Example: --kernels \"1:foo:bar:2\" will profile any kernel\n",
      "                        whose name contains \"bar\" and is the 2nd instance on context\n",
      "                        1 and on stream named \"foo\".\n",
      "\n",
      "  -m,  --metrics <metric names>\n",
      "                        Specify the metrics to be profiled on certain device(s).\n",
      "                        Multiple metric names separated by comma can be specified.\n",
      "                        Which device(s) are profiled is controlled by the \"--devices\"\n",
      "                        option. Otherwise metrics will be collected on all devices.\n",
      "                        For a list of available metrics, use \"--query-metrics\".\n",
      "                        Use \"--metrics all\" to profile all metrics available for\n",
      "                        each device.\n",
      "                        Use \"--devices\" and \"--kernels\" to select a specific kernel\n",
      "                        invocation. \n",
      "                        Note: \"--metrics all\" does not include some metrics which\n",
      "                        are needed for Visual Profiler's source level analysis.\n",
      "                        For that, use \"--analysis-metrics\".\n",
      "\n",
      "       --pc-sampling-period <period>\n",
      "                        Specify PC Sampling period in cycles,  at which the sampling\n",
      "                        records will be dumped. Allowed values for the period are\n",
      "                        integers between 5 to 31 both inclusive.\n",
      "                        This will set the sampling period to (2^period) cycles\n",
      "                        Default value is a number between 5 and 12 based on the setup.\n",
      "                        Note: Only available for GM20X+.\n",
      "                        \n",
      "\n",
      "       --profile-all-processes\n",
      "                        Profile all processes launched by the same user who launched\n",
      "                        this nvprof instance. Note: Only one instance of nvprof\n",
      "                        can run with this option at the same time. Under this mode,\n",
      "                        there's no need to specify an application to run.\n",
      "\n",
      "       --profile-api-trace <none|runtime|driver|all>\n",
      "                        Turn on/off CUDA runtime/driver API tracing. Allowed values:\n",
      "                        \tnone - turn off API tracing\n",
      "                        \truntime - only turn on CUDA runtime API tracing\n",
      "                        \tdriver - only turn on CUDA driver API tracing\n",
      "                        \tall - turn on all API tracing (default)\n",
      "\n",
      "       --profile-child-processes\n",
      "                        Profile the application and all child processes launched\n",
      "                        by it.\n",
      "\n",
      "       --profile-from-start <on|off>\n",
      "                        Enable/disable profiling from the start of the application.\n",
      "                        If it's disabled, the application can use {cu,cuda}Profiler{Start,Stop}\n",
      "                        to turn on/off profiling. Allowed values:\n",
      "                        \ton - enable profiling from start (default)\n",
      "                        \toff - disable profiling from start\n",
      "\n",
      "       --profiling-semaphore-pool-size <count>\n",
      "                        Set the profiling semaphore pool size reserved for storing\n",
      "                        profiling data for serialized kernels and memory operations\n",
      "                        for each context. The default value is 65536. The size should\n",
      "                        be a positive integer.\n",
      "\n",
      "       --query-events\n",
      "                        List all the events available on the device(s). Device(s)\n",
      "                        queried can be controlled by the \"--devices\" option.\n",
      "\n",
      "       --query-metrics\n",
      "                        List all the metrics available on the device(s). Device(s)\n",
      "                        queried can be controlled by the \"--devices\" option.\n",
      "\n",
      "       --replay-mode <mode>\n",
      "                        Choose replay mode used when not all events/metrics can be\n",
      "                        collected in a single run. Allowed values:\n",
      "                        \tdisabled - replay is disabled, events/metrics couldn't\n",
      "                        be profiled will be dropped\n",
      "                        \tkernel - each kernel invocation is replayed (default)\n",
      "                        \tapplication - the entire application is replayed.\n",
      "                        This mode is incompatible with \"--profile-all-processes\"\n",
      "                        or \"profile-child-processes\".\n",
      "\n",
      "       --skip-kernel-replay-save-restore <on|off>\n",
      "                        If enabled, this option can vastly improve kernel replay\n",
      "                        speed, as save and restore of the mutable state for each\n",
      "                        kernel pass will be skipped.\n",
      "                        Skipping of save/restore of input/output buffers allows you\n",
      "                        to specify that all profiled kernels on the context do not\n",
      "                        change the contents of their input buffers during execution,\n",
      "                        or call device malloc/free or new/delete, that leave the\n",
      "                        device heap in a different state. Specifically, a kernel\n",
      "                        can malloc and free a buffer in the same launch, but it\n",
      "                        cannot call an unmatched malloc or an unmatched free. Note:\n",
      "                        incorrectly using this mode while one of the kernels does\n",
      "                        modify the input buffer or uses unmatched malloc/free will\n",
      "                        result in undefined behavior, including kernel execution\n",
      "                        failure and/or corrupted device data. Allowed values:\n",
      "                        \ton - skip save/restore of the input/output buffers\n",
      "                        \toff - save/restore input/output buffers for each\n",
      "                        kernel replay pass (default)\n",
      "\n",
      "  -a,  --source-level-analysis <source level analysis names>\n",
      "                        Specify the source level metrics to be profiled on a certain\n",
      "                        kernel invocation. Use \"--devices\" and \"--kernels\" to select\n",
      "                        a specific kernel invocation. Allowed values: one or more\n",
      "                        of the following, separated by commas\n",
      "                        \tglobal_access: global access\n",
      "                        \tshared_access: shared access\n",
      "                        \tbranch: divergent branch\n",
      "                        \tinstruction_execution: instruction execution\n",
      "                        \tpc_sampling: pc sampling, available only for GM20X+\n",
      "                        Note: Use \"--export-profile\" to specify an export file.\n",
      "\n",
      "       --system-profiling <on|off>\n",
      "                        Turn on/off power, clock, and thermal profiling. Allowed\n",
      "                        values:\n",
      "                        \ton - turn on system profiling\n",
      "                        \toff - turn off system profiling (default)\n",
      "\n",
      "  -t,  --timeout <seconds>\n",
      "                        Set an execution timeout (in seconds) for the CUDA application.\n",
      "                        Note: Timeout starts counting from the moment the CUDA driver\n",
      "                        is initialized. If the application doesn't call any CUDA\n",
      "                        APIs, timeout won't be triggered.\n",
      "\n",
      "       --trace <gpu|api>\n",
      "                        Specify the option (or options seperated by commas) to be\n",
      "                        traced. Allowed values:\n",
      "                        \tapi - only turn on CUDA runtime and driver API tracing\n",
      "                        \tgpu - only turn on CUDA GPU tracing\n",
      "\n",
      "       --track-memory-allocations <on|off>\n",
      "                        Turn on/off tracking of memory operations, which involves\n",
      "                        recording timestamps, memory size, memory type and program\n",
      "                        counters of the memory allocations and frees. Turning this\n",
      "                        option on may incur an overhead during profiling. Allowed\n",
      "                        values:\n",
      "                        \ton - turn on tracking of memory allocations and\n",
      "                        free\n",
      "                        \toff - turn off tracking of memory allocations and\n",
      "                        free (default)\n",
      "\n",
      "       --unified-memory-profiling <per-process-device|off>\n",
      "                        Configure unified memory profiling. Allowed values:\n",
      "                        \tper-process-device - collect counts for each process\n",
      "                        and each device (default)\n",
      "                        \toff - turn off unified memory profiling\n",
      "\n",
      "       --cpu-profiling <on|off>\n",
      "                        Turn on CPU profiling. Note: CPU profiling is not supported\n",
      "                        in multi-process mode.\n",
      "\n",
      "       --cpu-profiling-explain-ccff <filename>\n",
      "                        Path to a PGI pgexplain.xml file that should be used to interpret\n",
      "                        Common Compiler Feedback Format (CCFF) messages.\n",
      "\n",
      "       --cpu-profiling-frequency <frequency>\n",
      "                        Set the CPU profiling frequency in samples per second. Default\n",
      "                        is 100Hz. Maximum is 500Hz.\n",
      "\n",
      "       --cpu-profiling-max-depth <depth>\n",
      "                        Set the maximum depth of each call stack. Zero means no limit.\n",
      "                        Default is zero.\n",
      "\n",
      "       --cpu-profiling-mode <flat|top-down|bottom-up>\n",
      "                        Set the output mode of CPU profiling. Allowed values:\n",
      "                        \tflat - Show flat profile\n",
      "                        \ttop-down - Show parent functions at the top\n",
      "                        \tbottom-up - Show parent functions at the bottom\n",
      "                        (default)\n",
      "\n",
      "       --cpu-profiling-percentage-threshold <threshold>\n",
      "                        Filter out the entries that are below the set percentage\n",
      "                        threshold. The limit should be an integer between 0 and\n",
      "                        100, inclusive. Zero means no limit. Default is zero.\n",
      "\n",
      "       --cpu-profiling-scope <function|instruction>\n",
      "                        Choose the profiling scope. Allowed values:\n",
      "                        \tfunction - Each level in the stack trace represents\n",
      "                        a distinct function (default)\n",
      "                        \tinstruction - Each level in the stack trace represents\n",
      "                        a distinct instruction address\n",
      "\n",
      "       --cpu-profiling-show-ccff <on|off>\n",
      "                        Choose whether to print Common Compiler Feedback Format (CCFF)\n",
      "                        messages embedded in the binary. Note: this option implies\n",
      "                        \"--cpu-profiling-scope instruction\". Default is off.\n",
      "\n",
      "       --cpu-profiling-show-library <on|off>\n",
      "                        Choose whether to print the library name for each sample.\n",
      "\n",
      "       --cpu-profiling-thread-mode <separated|aggregated>\n",
      "                        Set the thread mode of CPU profiling. Allowed values:\n",
      "                        \tseparated - Show separate profile for each thread\n",
      "                        \taggregated - Aggregate data from all threads (default)\n",
      "\n",
      "       --cpu-profiling-unwind-stack <on|off>\n",
      "                        Choose whether to unwind the CPU call-stack at each sample\n",
      "                        point. Default is on. \n",
      "\n",
      "       --openacc-profiling <on|off>\n",
      "                        Enable/disable recording information from the OpenACC profiling\n",
      "                        interface. Note: if the OpenACC profiling interface is available\n",
      "                        depends on the OpenACC runtime. Default is on.\n",
      "\n",
      "       --openmp-profiling <on|off>\n",
      "                        Enable/disable recording information from the OpenMP profiling\n",
      "                        interface. Note: if the OpenMP profiling interface is available\n",
      "                        depends on the OpenMP runtime. Default is off.\n",
      "\n",
      "       --context-name <name>\n",
      "                        Name of the CUDA context.\n",
      "                        \t\"%i\" in the context name string is replaced with\n",
      "                        the ID of the context.\n",
      "                        \t\"%p\" in the context name string is replaced with\n",
      "                        the process ID of the application being profiled.\n",
      "                        \t\"%q{<ENV>}\" in the context name string is replaced\n",
      "                        with the value of the environment variable \"<ENV>\". If the\n",
      "                        environment variable is not set it's an error.\n",
      "                        \t\"%h\" in the context name string is replaced with\n",
      "                        the hostname of the system.\n",
      "                        \t\"%%\" in the context name string is replaced with\n",
      "                        \"%\". Any other character following \"%\" is illegal.\n",
      "\n",
      "       --csv\n",
      "                        Use comma-separated values in the output.\n",
      "\n",
      "       --demangling <on|off>\n",
      "                        Turn on/off C++ name demangling of function names. Allowed\n",
      "                        values:\n",
      "                        \ton - turn on demangling (default)\n",
      "                        \toff - turn off demangling\n",
      "\n",
      "  -u,  --normalized-time-unit <s|ms|us|ns|col|auto>\n",
      "                        Specify the unit of time that will be used in the output.\n",
      "                        Allowed values:\n",
      "                        \ts - second, ms - millisecond, us - microsecond,\n",
      "                        ns - nanosecond\n",
      "                        \tcol - a fixed unit for each column\n",
      "                        \tauto (default) - the scale is chosen for each value\n",
      "                        based on its length.\n",
      "\n",
      "       --openacc-summary-mode <mode>\n",
      "                        Set how durations are computed in the OpenACC summary. Allowed\n",
      "                        values:\n",
      "                        \texclusive: show exclusive times (default)\n",
      "                        \tinclusive: show inclusive times\n",
      "\n",
      "       --print-api-summary\n",
      "                        Print a summary of CUDA runtime/driver API calls.\n",
      "\n",
      "       --print-api-trace\n",
      "                        Print CUDA runtime/driver API trace.\n",
      "\n",
      "       --print-dependency-analysis-trace\n",
      "                        Print dependency analysis trace.\n",
      "\n",
      "       --print-gpu-summary\n",
      "                        Print a summary of the activities on the GPU (including CUDA\n",
      "                        kernels and memcpy's/memset's).\n",
      "\n",
      "       --print-gpu-trace\n",
      "                        Print individual kernel invocations (including CUDA memcpy's/memset's)\n",
      "                        and sort them in chronological order. In event/metric profiling\n",
      "                        mode, show events/metrics for each kernel invocation.\n",
      "\n",
      "       --print-openacc-constructs\n",
      "                        Include parent construct names in OpenACC profile.\n",
      "\n",
      "       --print-openacc-summary\n",
      "                        Print a summary of the OpenACC profile.\n",
      "\n",
      "       --print-openacc-trace\n",
      "                        Print a trace of the OpenACC profile.\n",
      "\n",
      "       --print-openmp-summary\n",
      "                        Print a summary of the OpenMP profile.\n",
      "\n",
      "  -s,  --print-summary\n",
      "                        Print a summary of the profiling result on screen. Note:\n",
      "                        This is the default unless \"--export-profile\" or other print\n",
      "                        options are used.\n",
      "\n",
      "       --print-summary-per-gpu\n",
      "                        Print a summary of the profiling result for each GPU.\n",
      "\n",
      "       --process-name <name>\n",
      "                        Name of the process.\n",
      "                        \t\"%p\" in the process name string is replaced with\n",
      "                        the process ID of the application being profiled.\n",
      "                        \t\"%q{<ENV>}\" in the process name string is replaced\n",
      "                        with the value of the environment variable \"<ENV>\". If the\n",
      "                        environment variable is not set it's an error.\n",
      "                        \t\"%h\" in the process name string is replaced with\n",
      "                        the hostname of the system.\n",
      "                        \t\"%%\" in the process  name string is replaced with\n",
      "                        \"%\". Any other character following \"%\" is illegal.\n",
      "\n",
      "       --quiet\n",
      "                        Suppress all nvprof output.\n",
      "\n",
      "       --stream-name <name>\n",
      "                        Name of the CUDA stream.\n",
      "                        \t\"%i\" in the stream name string is replaced with the\n",
      "                        ID of the stream.\n",
      "                        \t\"%p\" in the stream name string is replaced with\n",
      "                        the process ID of the application being profiled.\n",
      "                        \t\"%q{<ENV>}\" in the stream name string is replaced\n",
      "                        with the value of the environment variable \"<ENV>\". If the\n",
      "                        environment variable is not set it's an error.\n",
      "                        \t\"%h\" in the stream name string is replaced with\n",
      "                        the hostname of the system.\n",
      "                        \t\"%%\" in the stream name string is replaced with\n",
      "                        \"%\". Any other character following \"%\" is illegal.\n",
      "\n",
      "  -o,  --export-profile <filename>\n",
      "                        Export the result file which can be imported later or opened\n",
      "                        by the NVIDIA Visual Profiler.\n",
      "                        \t\"%p\" in the file name string is replaced with the\n",
      "                        process ID of the application being profiled.\n",
      "                        \t\"%q{<ENV>}\" in the file name string is replaced\n",
      "                        with the value of the environment variable \"<ENV>\". If the\n",
      "                        environment variable is not set it's an error.\n",
      "                        \t\"%h\" in the file name string is replaced with the\n",
      "                        hostname of the system.\n",
      "                        \t\"%%\" in the file name string is replaced with \"%\".\n",
      "                        \tAny other character following \"%\" is illegal.\n",
      "                        By default, this option disables the summary output. Note:\n",
      "                        If the application being profiled creates child processes,\n",
      "                        or if '--profile-all-processes' is used, the \"%p\" format\n",
      "                        is needed to get correct export files for each process.\n",
      "\n",
      "  -f,  --force-overwrite\n",
      "                        Force overwriting all output files (any existing files will\n",
      "                        be overwritten).\n",
      "\n",
      "  -i,  --import-profile <filename>\n",
      "                        Import a result profile from a previous run.\n",
      "\n",
      "       --log-file <filename>\n",
      "                        Make nvprof send all its output to the specified file, or\n",
      "                        one of the standard channels. The file will be overwritten.\n",
      "                        If the file doesn't exist, a new one will be created.\n",
      "                        \t\"%1\" as the whole file name indicates standard output\n",
      "                        channel (stdout).\n",
      "                        \t\"%2\" as the whole file name indicates standard error\n",
      "                        channel (stderr). Note: This is the default.\n",
      "                        \t\"%p\" in the file name string is replaced with the\n",
      "                        process ID of the application being profiled.\n",
      "                        \t\"%q{<ENV>}\" in the file name string is replaced\n",
      "                        with the value of the environment variable \"<ENV>\". If the\n",
      "                        environment variable is not set it's an error.\n",
      "                        \t\"%h\" in the file name string is replaced with the\n",
      "                        hostname of the system.\n",
      "                        \t\"%%\" in the file name is replaced with \"%\".\n",
      "                        \tAny other character following \"%\" is illegal.\n",
      "\n",
      "       --print-nvlink-topology\n",
      "                        Print NVLink topology \n",
      "\n",
      "       --print-pci-topology\n",
      "                        Print PCI topology\n",
      "\n",
      "  -h,  --help\n",
      "                        Print this help information.\n",
      "\n",
      "  -V,  --version\n",
      "                        Print version information of this tool.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvprof --help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
